{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook  regards the classification model used for my capstone project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The machine learning model I use is RandomForestClassifier of the lib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.config.option_context at 0x1a1c10dc50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import string \n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk_sents\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "pd.option_context('display.max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('job_skills.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1235, 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 20 rows containing NaNs in the dataset which are dropped\n",
    "data2 = data.dropna()\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to build a column of labels (otherwise called targets) the Category field of my dataset is encoded in numbers form 0 to 10. Here I'm encoding the first ten most large categories while considering the remaining categories which are less important as a unique category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_category= []\n",
    "categories = {}\n",
    "\n",
    "for i in data2['Category']:\n",
    "    if 'Sales & Account Management' in i:\n",
    "        my_category.append(0)\n",
    "        categories[0] = \"Sales & Account Management\"     #thisdict[\"color\"] = \"red\"\n",
    "        continue\n",
    "        \n",
    "    if 'Marketing & Communications' in i :\n",
    "        my_category.append(1)\n",
    "        categories[1] = \"Marketing & Communications\"\n",
    "        continue\n",
    "        \n",
    "    if 'Finance' in i :\n",
    "        my_category.append(2)\n",
    "        categories[2] = \"Finance\"\n",
    "        continue\n",
    "        \n",
    "    if 'Technical Solutions' in i :\n",
    "        my_category.append(3)  \n",
    "        categories[3] = \"Technical Solutions\"\n",
    "        continue\n",
    "        \n",
    "    if 'Business Strategy' in i :\n",
    "        my_category.append(4) \n",
    "        categories[4] = \"Business Strategy\"\n",
    "        continue\n",
    "        \n",
    "    if 'People Operations' in i:\n",
    "        my_category.append(5)\n",
    "        categories[5] = \"People Operations\"\n",
    "        continue\n",
    "        \n",
    "    if 'User Experience & Design' in i:\n",
    "        my_category.append(6)\n",
    "        categories[6] = \"User Experience & Design\"\n",
    "        continue\n",
    "        \n",
    "    if 'Program Management' in i:\n",
    "        my_category.append(7)\n",
    "        categories[7] = \"Program Management\"\n",
    "        continue\n",
    "        \n",
    "    if 'Partnerships' in i:\n",
    "        my_category.append(8)\n",
    "        categories[8] = \"Partnerships\"\n",
    "        continue\n",
    "        \n",
    "    if 'Product & Customer Support' in i:\n",
    "        my_category.append(9)\n",
    "        categories[9] = \"Product & Customer Support\"\n",
    "        \n",
    "    else:\n",
    "        my_category.append(10)\n",
    "        categories[10] = \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the column 'My_category' as a label to the dataste\n",
    "data2.insert(loc=7, column='My_category', value=my_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: 'Program Management',\n",
       " 10: 'Other',\n",
       " 3: 'Technical Solutions',\n",
       " 8: 'Partnerships',\n",
       " 9: 'Product & Customer Support',\n",
       " 4: 'Business Strategy',\n",
       " 1: 'Marketing & Communications',\n",
       " 0: 'Sales & Account Management',\n",
       " 2: 'Finance',\n",
       " 6: 'User Experience & Design',\n",
       " 5: 'People Operations'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Location</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Minimum Qualifications</th>\n",
       "      <th>Preferred Qualifications</th>\n",
       "      <th>My_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>Google Cloud Program Manager</td>\n",
       "      <td>Program Management</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Shape, shepherd, ship, and show technical prog...</td>\n",
       "      <td>BA/BS degree or equivalent practical experienc...</td>\n",
       "      <td>Experience in the business technology market a...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>Supplier Development Engineer (SDE), Cable/Con...</td>\n",
       "      <td>Manufacturing &amp; Supply Chain</td>\n",
       "      <td>Shanghai, China</td>\n",
       "      <td>Drive cross-functional activities in the suppl...</td>\n",
       "      <td>BS degree in an Engineering discipline or equi...</td>\n",
       "      <td>BSEE, BSME or BSIE degree.\\nExperience of usin...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company                                              Title  \\\n",
       "0  Google                       Google Cloud Program Manager   \n",
       "1  Google  Supplier Development Engineer (SDE), Cable/Con...   \n",
       "\n",
       "                       Category         Location  \\\n",
       "0            Program Management        Singapore   \n",
       "1  Manufacturing & Supply Chain  Shanghai, China   \n",
       "\n",
       "                                    Responsibilities  \\\n",
       "0  Shape, shepherd, ship, and show technical prog...   \n",
       "1  Drive cross-functional activities in the suppl...   \n",
       "\n",
       "                              Minimum Qualifications  \\\n",
       "0  BA/BS degree or equivalent practical experienc...   \n",
       "1  BS degree in an Engineering discipline or equi...   \n",
       "\n",
       "                            Preferred Qualifications  My_category  \n",
       "0  Experience in the business technology market a...            7  \n",
       "1  BSEE, BSME or BSIE degree.\\nExperience of usin...           10  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here a small print-out just to check the label column is created correctly\n",
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization : it is a way of uncovering the underlying structure of the dataset. This structure can then be learned and used to identify similar structures in other dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a way of vectorization I'm using a Term Frequency - Inverse Document Frequency Matrix implemented in TfidfVectorizer class of sklearn \n",
    "\n",
    "#### The TF-IDF creates a document term matrix, where there's one row per text and the columns represent single unique terms. But the cells do not represent the count, they represent instead a weighting that's meant to identify how important a word is to an individual text. The formula below shows how this weighting is computed. \n",
    "\n",
    "<img src=\"files/tfidf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to build the TF-IDF matrix the following steps will be performed:\n",
    "#### 1- The text in 'Responsibilities' column in the dataset will be tokenized, punctuation will be removed, il will     be stemed in order to find the root of the words and stop words will be also removed\n",
    "#### 2- I have implemented a function that does just that-\n",
    "#### 3- the function will then be passed to the class TfidfVectorizer that will use it to (see 4)\n",
    "#### 4- create the TF-IDF matrix by using the fit_transform() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_txt(tex):\n",
    "    tex_no_pun =\"\".join([char for char in tex if char not in string.punctuation])\n",
    "    tokens = re.split('\\W+', tex_no_pun)\n",
    "    tex_no_pun = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return tex_no_pun\n",
    "\n",
    "sentences = []\n",
    "for i in data2['Responsibilities']:\n",
    "    li = clean_txt(i)\n",
    "    sentences.append(li)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1235, 2775)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "['', '10q10k', '10x', '11', '1tomani', '20', '247', '24x7', '2D', '2d3d', '30', '3D', '3plasp', '3pr', '40', '50', 'AB', 'AM', 'AR', 'AV', 'Ad', 'As', 'BD', 'BI', 'Be', 'C', 'CA', 'CC', 'CE', 'CM', 'CX', 'DC', 'EU', 'FA', 'FX', 'G', 'Go', 'HR', 'IA', 'IC', 'IN', 'IP', 'IT', 'If', 'In', 'MA', 'ML', 'OS', 'PA', 'PD', 'PM', 'PR', 'QA', 'SP', 'TV', 'UI', 'UK', 'UL', 'US', 'UX', 'VP', 'VR', 'abil', 'abl', 'abreast', 'abroad', 'absenc', 'abstract', 'abus', 'abusespam', 'academia', 'acceler', 'accept', 'acceptancedeclin', 'access', 'accessori', 'accommod', 'accommodationsaccess', 'accomplish', 'accord', 'accordingli', 'account', 'accountterritori', 'accrual', 'accur', 'accuraci', 'accuratetlmfunct', 'achiev', 'achiv', 'acquir', 'acquisit', 'across', 'act', 'action', 'actionsimprov', 'activ', 'actual', 'acumen', 'ad', 'adapt', 'add', 'addit', 'address', 'adequ', 'adher', 'adhoc', 'adjust', 'administ', 'administr', 'admiss', 'admob', 'adopt', 'adoptionrel', 'adoptionusag', 'adsens', 'advanc', 'advantag', 'advers', 'advert', 'advertis', 'advertisersag', 'advertisingrel', 'advic', 'advis', 'advisor', 'advisori', 'advoc', 'advocaci', 'adword', 'adwordsrel', 'aesthet', 'affair', 'affect', 'affili', 'africa', 'age', 'agenc', 'agenciesbrand', 'agenciespartn', 'agencylevel', 'agenda', 'agendaprior', 'agent', 'aggress', 'agre', 'agreeabl', 'agreement', 'ahead', 'aid', 'aim', 'alert', 'algorithm', 'alias', 'align', 'alik', 'alleg', 'allegro', 'allianc', 'alloc', 'allocationdistribut', 'allow', 'along', 'alongsid', 'alphabet', 'also', 'altern', 'alumni', 'alway', 'ambassador', 'ambigu', 'ambit', 'ambiti', 'amer', 'america', 'amidst', 'aml', 'among', 'amongst', 'amount', 'amp', 'amplif', 'analog', 'analys', 'analysi', 'analysismodel', 'analysisreport', 'analyst', 'analyt', 'analyticsdriven', 'analyz', 'analyzeoptim', 'analyzingmanag', 'andor', 'android', 'angl', 'annot', 'announc', 'annual', 'answer', 'antiabus', 'anticip', 'anticipateresolv', 'anvato', 'apac', 'api', 'apige', 'app', 'appdev', 'appli', 'applic', 'appoint', 'appreci', 'appris', 'approach', 'appropri', 'appropriaterelev', 'approv', 'architect', 'architectur', 'area', 'aris', 'around', 'arrang', 'array', 'art', 'articl', 'articul', 'artifact', 'artifici', 'artist', 'asia', 'asian', 'asic', 'ask', 'aspect', 'assembl', 'assert', 'assess', 'asset', 'assign', 'assist', 'associ', 'assum', 'assumpt', 'assur', 'attack', 'attain', 'attend', 'attent', 'attorney', 'attract', 'attribut', 'atyp', 'auction', 'audienc', 'audio', 'audiovisu', 'audit', 'auditor', 'author', 'auto', 'autom', 'automationtool', 'automot', 'autonom', 'avail', 'avoid', 'awar', 'b2b', 'back', 'backend', 'background', 'backtest', 'bahasa', 'balanc', 'bank', 'banner', 'bar', 'base', 'basi', 'batteri', 'bay', 'beauti', 'becom', 'begin', 'behalf', 'behavior', 'behaviour', 'behind', 'belaru', 'benchmark', 'benefici', 'benefit', 'berlin', 'bespok', 'best', 'bestinclass', 'bestpractic', 'bet', 'beta', 'betspecif', 'better', 'beyond', 'bid', 'big', 'bigger', 'biggest', 'bill', 'biospecimen', 'blameless', 'blend', 'block', 'blocker', 'blog', 'blueprint', 'board', 'boardlevel', 'bom', 'bonu', 'book', 'boost', 'born', 'bottom', 'boundari', 'box', 'bpo', 'brand', 'brandexperi', 'brandlab', 'brandproduct', 'brazil', 'breach', 'break', 'breakthrough', 'bridg', 'brief', 'bring', 'bringup', 'broad', 'broadbas', 'broadcast', 'broader', 'broadscal', 'broker', 'browser', 'bu', 'budget', 'budgetari', 'budgetsaccount', 'bug', 'bugsfeatur', 'build', 'built', 'builtin', 'bulk', 'bundl', 'burden', 'bureau', 'busi', 'businessaccount', 'businessadoptionrel', 'businessag', 'businesscrit', 'businesscustom', 'businessrelev', 'businesss', 'buy', 'buyer', 'buyin', 'cadenc', 'cafe', 'calcul', 'calendar', 'calibr', 'call', 'camera', 'campaign', 'campu', 'cancel', 'candid', 'cantones', 'canva', 'capabl', 'capac', 'captur', 'card', 'care', 'career', 'carri', 'carrier', 'cascad', 'case', 'cash', 'casten', 'catalog', 'catalyst', 'catch', 'categori', 'caus', 'cblevel', 'celebr', 'cell', 'cement', 'center', 'central', 'ceqanepa', 'certif', 'chain', 'chainoftitl', 'challeng', 'chamber', 'champion', 'chang', 'channel', 'channelatl', 'channelswid', 'character', 'charg', 'charm', 'charter', 'chat', 'check', 'checklist', 'chief', 'child', 'children', 'china', 'chines', 'chip', 'choic', 'chrome', 'chromebook', 'chromecast', 'ci', 'cicd', 'circuit', 'circumst', 'citi', 'claim', 'clarifi', 'clariti', 'class', 'classroom', 'clean', 'cleanup', 'clear', 'clearanc', 'clearli', 'clevel', 'client', 'clientfac', 'climat', 'clock', 'close', 'closedloop', 'closur', 'cloud', 'cloudbas', 'cluster', 'clutterbreak', 'cmf', 'cmodm', 'coach', 'coat', 'code', 'codec', 'codevelop', 'codriv', 'coexist', 'cohes', 'cohort', 'collabor', 'collat', 'collater', 'colleagu', 'collect', 'color', 'comarket', 'combat', 'combin', 'come', 'comfort', 'command', 'commerc', 'commerci', 'commissari', 'commit', 'committe', 'commod', 'common', 'commonli', 'commonwealth', 'commun', 'communitycoordin', 'comp', 'compani', 'companywid', 'compat', 'compel', 'compellingli', 'compens', 'compet', 'competit', 'compil', 'complaint', 'complement', 'complet', 'complex', 'compli', 'complianc', 'compliant', 'compon', 'composit', 'comprehend', 'comprehens', 'compress', 'comput', 'conceiv', 'concept', 'conceptu', 'concern', 'concis', 'conclud', 'conclus', 'concret', 'concurr', 'condit', 'conduct', 'conduit', 'confer', 'conferenc', 'confidenti', 'configur', 'confirm', 'conflict', 'conjunct', 'connect', 'consensu', 'consid', 'consider', 'consist', 'consol', 'consolid', 'constant', 'constantli', 'constitu', 'constraint', 'construct', 'consult', 'consum', 'consumerfocus', 'consumpt', 'contact', 'contactopportun', 'contain', 'content', 'contentgraph', 'contentrel', 'contest', 'context', 'contin', 'conting', 'continu', 'contrabusi', 'contract', 'contractor', 'contractu', 'contrarevenu', 'contribut', 'contributor', 'control', 'controllership', 'controlreduct', 'controversi', 'convers', 'convert', 'convey', 'convinc', 'cook', 'cooper', 'coordin', 'copi', 'copromot', 'copyphoto', 'copyright', 'core', 'corpor', 'correct', 'correspond', 'cost', 'costbenefit', 'costeffect', 'could', 'council', 'counsel', 'counselsupport', 'counterpart', 'countri', 'coupl', 'cours', 'coursecorrect', 'cover', 'coverag', 'cpfr', 'cpg', 'cpu', 'craft', 'creat', 'createchang', 'createedit', 'createmaintain', 'creation', 'creativ', 'creativeuniqu', 'creator', 'credibl', 'credit', 'crew', 'crime', 'crise', 'criteria', 'critic', 'critiqu', 'crm', 'cross', 'crosschannel', 'crosscollabor', 'crosscompani', 'crosscultur', 'crossfin', 'crossfunct', 'crossfunctionalcrossregion', 'crossgroup', 'crossmarket', 'crossmedia', 'crossoffic', 'crossorganiz', 'crossplatform', 'crossproduct', 'crosspromot', 'crossteam', 'crystal', 'csat', 'css', 'cto', 'cultiv', 'cultur', 'curat', 'curios', 'currenc', 'current', 'curricular', 'curriculum', 'custom', 'customercentr', 'customerfac', 'customerfocus', 'customerpartn', 'cut', 'cuttingedg', 'cxo', 'cycl', 'dach', 'daili', 'dashboard', 'data', 'databas', 'datadriven', 'datainform', 'dataset', 'date', 'day', 'daytoday', 'dbm', 'ddm', 'deadlin', 'deal', 'dean', 'debat', 'debrief', 'debt', 'debug', 'decid', 'decis', 'decisionmak', 'decompos', 'dedic', 'deep', 'deepdiv', 'deepen', 'deeper', 'defam', 'defect', 'defens', 'defer', 'defici', 'defin', 'definedevelop', 'definit', 'definitiondesign', 'degre', 'delay', 'delight', 'deliv', 'deliver', 'deliverablescommit', 'deliveri', 'demand', 'demo', 'demonstr', 'densiti', 'depart', 'depend', 'deploy', 'depth', 'deriv', 'describ', 'descript', 'design', 'designbuildrefin', 'designroadmap', 'desir', 'desk', 'despit', 'detail', 'detect', 'determin', 'develop', 'development', 'developmentstrend', 'deviat', 'devic', 'deviceplatform', 'devis', 'dfm', 'diagnos', 'diagnosi', 'diagnost', 'diagram', 'dialog', 'dialogu', 'differ', 'differenti', 'difficult', 'digit', 'digniti', 'dilig', 'dimens', 'diplomaci', 'direct', 'directindirect', 'directli', 'director', 'directori', 'disabl', 'disast', 'disciplin', 'disclosur', 'discov', 'discoveri', 'discuss', 'discussionsreview', 'dispar', 'display', 'displayyoutub', 'dissemin', 'distil', 'distribut', 'distributor', 'district', 'disturb', 'dive', 'divers', 'divis', 'doc', 'document', 'documentationexecut', 'doe', 'domain', 'domest', 'done', 'door', 'dot', 'doubleclick', 'downstream', 'draft', 'dramat', 'draw', 'drawingsketch', 'drill', 'drive', 'driven', 'driver', 'drumbeat', 'due', 'duo', 'duti', 'dvf', 'dynam', 'ear', 'earli', 'earn', 'easi', 'easili', 'east', 'eauction', 'econom', 'ecosystem', 'ed', 'edg', 'edit', 'editor', 'editori', 'edm', 'educ', 'effect', 'efficaci', 'effici', 'efficiencyeffect', 'effort', 'eg', 'either', 'elearn', 'elect', 'electr', 'electrod', 'electrolyt', 'element', 'elev', 'elimin', 'em', 'email', 'embrac', 'emea', 'emeaglob', 'emeawid', 'emerg', 'empathet', 'emphas', 'emphasi', 'employ', 'employe', 'employmentrel', 'empow', 'empti', 'emul', 'enabl', 'encount', 'encourag', 'end', 'endtoend', 'endus', 'energ', 'energi', 'energyrel', 'enforc', 'eng', 'engag', 'engin', 'engineeringfocus', 'engineeringlevel', 'english', 'enhanc', 'enjoy', 'enlighten', 'enquiri', 'enrol', 'ensur', 'enterpris', 'enterprisefocus', 'entertain', 'entir', 'entiti', 'entitl', 'entri', 'environ', 'environment', 'equip', 'equipmentfixtur', 'equit', 'equiti', 'er', 'ergonom', 'error', 'escal', 'escalationsfeedback', 'especi', 'essenc', 'essenti', 'establish', 'estat', 'estim', 'etc', 'ethic', 'ethnographi', 'etl', 'europ', 'european', 'evacu', 'evalu', 'evangel', 'evangelis', 'even', 'event', 'eventu', 'everchang', 'everi', 'everyon', 'everyth', 'evolut', 'evolutionari', 'evolv', 'examin', 'exampl', 'exceed', 'excel', 'except', 'exchang', 'excit', 'execut', 'executivelevel', 'exercis', 'exist', 'expand', 'expans', 'expect', 'expedit', 'expediti', 'expenditur', 'expens', 'experi', 'experienti', 'experiment', 'expert', 'expertis', 'explain', 'exploit', 'explor', 'exploratori', 'export', 'exposur', 'express', 'extend', 'extens', 'extern', 'externalfac', 'eye', 'face', 'facetofac', 'facil', 'facilit', 'factfind', 'factor', 'factori', 'factual', 'failur', 'fairli', 'familiar', 'fan', 'fashion', 'fast', 'faster', 'fastpac', 'fault', 'fcc', 'fda', 'feasibl', 'featur', 'featureformat', 'feder', 'feed', 'feedback', 'feel', 'fiber', 'field', 'fight', 'figur', 'file', 'fill', 'filter', 'final', 'finalist', 'financ', 'financecontrollership', 'financelevel', 'financi', 'financialcommerci', 'find', 'finish', 'fire', 'firebas', 'firewal', 'first', 'firstofitskind', 'fit', 'fix', 'fixtur', 'flag', 'flawless', 'flawlessli', 'fleet', 'flexibl', 'flight', 'flow', 'flowchart', 'fluid', 'fmcg', 'fmea', 'focu', 'focus', 'follow', 'followthesun', 'followup', 'food', 'footnot', 'forc', 'forecast', 'forecastbudget', 'forecastsspend', 'forefront', 'foreign', 'forens', 'form', 'formal', 'format', 'formul', 'forum', 'forward', 'forwardthink', 'foster', 'found', 'fpa', 'fpga', 'frame', 'framework', 'fraud', 'fraudabus', 'fraudirregular', 'fraudster', 'free', 'freelanc', 'french', 'frequenc', 'frequent', 'fresh', 'friction', 'front', 'frontend', 'frontlin', 'fuel', 'fulfil', 'full', 'fullchip', 'fulli', 'fullyfeatur', 'function', 'fund', 'fundament', 'funnel', 'furnitur', 'futur', 'ga360', 'gaap', 'gain', 'game', 'gap', 'garner', 'gate', 'gather', 'gaug', 'gca', 'gce', 'gcp', 'gcpfocus', 'gdn', 'gdpr', 'gdprrelat', 'gener', 'generationrel', 'genuin', 'geograph', 'geographi', 'german', 'germani', 'get', 'gfa', 'gift', 'give', 'giveaway', 'given', 'global', 'globalapac', 'globalregion', 'globe', 'gm', 'gmsmanag', 'go', 'goal', 'goalset', 'good', 'googl', 'googlecalib', 'googlecultiv', 'googledoubleclick', 'googler', 'googlersmanag', 'googlespecif', 'googlewid', 'googley', 'gotomarket', 'govern', 'gp', 'gpl', 'gpse', 'grade', 'grammar', 'graphic', 'great', 'greater', 'green', 'grid', 'grosstonet', 'ground', 'groundup', 'group', 'grow', 'growth', 'gsi', 'gsr', 'gsuit', 'gtm', 'guarante', 'guard', 'guid', 'guidanc', 'guidelin', 'guidelinespolici', 'hackathon', 'hand', 'handl', 'handov', 'handson', 'happi', 'hardwar', 'hardwaresoftwar', 'harm', 'have', 'head', 'headcount', 'health', 'healthcar', 'healthi', 'healthsafeti', 'hear', 'hedg', 'heighten', 'help', 'henc', 'hfm', 'high', 'higher', 'highest', 'highimpact', 'highlevel', 'highli', 'highlight', 'highlyst', 'highpac', 'highperform', 'highpotenti', 'highprofil', 'highqual', 'highrisk', 'hightouch', 'hightrust', 'highvalu', 'highvolum', 'hindi', 'hire', 'histor', 'histori', 'hit', 'hl', 'hoc', 'hold', 'holist', 'home', 'homegrown', 'honest', 'hong', 'horizont', 'host', 'hotel', 'hour', 'hpc', 'hrbp', 'hrrelat', 'htcondor', 'html', 'huge', 'hundr', 'hybrid', 'hyperion', 'hypothes', 'hypothesisdriven', 'icfr', 'icon', 'iconographi', 'idea', 'ideal', 'ideat', 'ident', 'identif', 'identifi', 'identifylead', 'identifypriorit', 'identifytroubleshoot', 'ie', 'ifc', 'ifr', 'ignit', 'illustr', 'imag', 'imageri', 'immedi', 'immers', 'impact', 'impart', 'impedi', 'implement', 'implementationlaunch', 'implic', 'import', 'improv', 'inappropri', 'inbound', 'incent', 'incept', 'incid', 'incl', 'includ', 'inclus', 'incom', 'inconsist', 'incorpor', 'incountri', 'increas', 'incred', 'increment', 'incub', 'independ', 'indepth', 'india', 'indic', 'indirect', 'individu', 'indonesia', 'industri', 'industrycli', 'industryecosystem', 'industryfocus', 'industryspecif', 'influenc', 'inform', 'informationproposalsquot', 'infract', 'infrastructur', 'infring', 'ingam', 'inhous', 'initi', 'injuri', 'inlif', 'inmarket', 'innov', 'inperson', 'inproduct', 'input', 'inputrequir', 'inquiri', 'inregion', 'insert', 'insid', 'insight', 'insist', 'inspect', 'inspir', 'instal', 'instil', 'institut', 'instor', 'instruct', 'instructor', 'instructorl', 'instrument', 'instrumentrel', 'insur', 'integr', 'integrationproduct', 'integratorservic', 'intellectu', 'intellig', 'intend', 'intens', 'intent', 'interact', 'interconnect', 'interdepend', 'interest', 'interfac', 'intermediari', 'intermediateterm', 'intern', 'internalextern', 'internallyextern', 'internet', 'internship', 'interpret', 'intersect', 'interview', 'introduc', 'introduct', 'introductori', 'invalid', 'invent', 'inventori', 'invest', 'investdivest', 'investig', 'invoic', 'involv', 'io', 'ip', 'ipe', 'ipv4', 'ipv6', 'israel', 'issu', 'issuesescal', 'isv', 'item', 'iter', 'itgc', 'itrel', 'janitori', 'japac', 'japan', 'japanes', 'java', 'javascript', 'jdm', 'jigsaw', 'job', 'join', 'joint', 'jointli', 'journal', 'journalist', 'journey', 'judgment', 'judici', 'junior', 'jurisdict', 'justifi', 'keep', 'key', 'keynot', 'keyword', 'kickoff', 'knack', 'know', 'knowledg', 'known', 'kong', 'korea', 'korean', 'kpi', 'laam', 'lab', 'label', 'labor', 'labour', 'ladder', 'lagginglead', 'land', 'landown', 'landscap', 'languag', 'larg', 'larger', 'larges', 'largescal', 'largest', 'last', 'latam', 'late', 'latenc', 'latest', 'latic', 'latin', 'launch', 'launchingrun', 'law', 'lawyer', 'layout', 'lc', 'lead', 'leadcontribut', 'leader', 'leadershead', 'leadership', 'leadersmanag', 'leansuppli', 'learn', 'learningengag', 'learningsdata', 'leas', 'leav', 'led', 'ledger', 'legal', 'legisl', 'lend', 'less', 'lesson', 'level', 'lever', 'leverag', 'liabil', 'liais', 'liaison', 'licens', 'licensesoth', 'life', 'lifecycl', 'lifesafeti', 'lift', 'light', 'lighthous', 'like', 'likelihood', 'limit', 'line', 'linguist', 'link', 'linux', 'liquid', 'list', 'listen', 'lithiumion', 'litig', 'live', 'load', 'lobbi', 'local', 'localregion', 'locat', 'log', 'logic', 'logist', 'long', 'longer', 'longerterm', 'longterm', 'look', 'loop', 'loophol', 'lot', 'love', 'lowlevel', 'lowpow', 'lp', 'lsf', 'machin', 'made', 'magic', 'main', 'mainland', 'mainstream', 'maintain', 'mainten', 'maintenanceclean', 'major', 'make', 'maker', 'manag', 'managelead', 'managementbas', 'managementengin', 'managersbusi', 'managerstag', 'mandarin', 'mandat', 'mani', 'manipul', 'manner', 'manual', 'manufactur', 'manufacturingassemblytest', 'map', 'margin', 'market', 'marketcompetitiveindustri', 'marketindustri', 'marketingadvertis', 'marketingprogram', 'marketo', 'marketplac', 'marketrelev', 'marri', 'mass', 'massiv', 'master', 'match', 'materi', 'matric', 'matrix', 'matter', 'matur', 'maxim', 'maximum', 'may', 'mea', 'mean', 'meaning', 'measur', 'mechan', 'media', 'medium', 'mediumlong', 'meet', 'melayu', 'member', 'memo', 'memori', 'mena', 'mentor', 'mentordevelop', 'mentorship', 'merchandis', 'merchant', 'merit', 'messag', 'met', 'metadata', 'method', 'methodolog', 'methodsequip', 'methodstool', 'metric', 'metrolog', 'microarchitectur', 'mid', 'midmarket', 'midsiz', 'migrat', 'mileston', 'million', 'mind', 'mindset', 'mine', 'minim', 'minimum', 'minor', 'misalign', 'mission', 'missioncrit', 'mitig', 'mix', 'mobil', 'mobilefocus', 'mock', 'mockup', 'mode', 'model', 'modelingtest', 'moder', 'modern', 'modif', 'modifi', 'modul', 'modular', 'mold', 'moment', 'momentum', 'monet', 'monitor', 'month', 'monthend', 'monthli', 'monthovermonth', 'moon', 'morn', 'mortem', 'motion', 'motiv', 'mountain', 'move', 'movement', 'multibrand', 'multichannel', 'multicountri', 'multidisciplinari', 'multifacet', 'multipl', 'multiplatform', 'multipleel', 'multiquart', 'multivendor', 'multiyear', 'munich', 'music', 'mutual', 'name', 'narr', 'nascent', 'nation', 'natur', 'navig', 'nbu', 'nearterm', 'neatli', 'necessari', 'need', 'needsrequir', 'neg', 'negoti', 'nest', 'net', 'network', 'networkimpact', 'new', 'newest', 'newrenew', 'newuniqu', 'next', 'nextgener', 'nf', 'nich', 'nlu', 'node', 'nomenclatur', 'nonad', 'nonexcept', 'nonexpert', 'nonfin', 'nonleg', 'nonoffic', 'nonpayrol', 'nonprofit', 'nonroutin', 'nonsal', 'nonstandard', 'nontechn', 'nonu', 'nonwrit', 'noogler', 'norm', 'north', 'note', 'notetak', 'notic', 'notif', 'np', 'npi', 'nuanc', 'number', 'nurtur', 'object', 'oblig', 'obstacl', 'obtain', 'obtainprovid', 'occur', 'oem', 'offens', 'offer', 'offic', 'offici', 'offlin', 'offshor', 'often', 'okr', 'oktoship', 'onboard', 'onbrand', 'oncal', 'ondevic', 'one', 'oneoff', 'oneonon', 'onepag', 'onetomani', 'ongo', 'onlin', 'onlinerma', 'onpremis', 'onscreen', 'onshor', 'onsit', 'ontim', 'op', 'ope', 'open', 'oper', 'operation', 'opg', 'opinion', 'opportun', 'optic', 'optim', 'optimis', 'optimum', 'option', 'oqc', 'oral', 'orcad', 'orchestr', 'order', 'organ', 'organis', 'organiz', 'organizationalpeoplerel', 'origin', 'osrel', 'other', 'our', 'out', 'outag', 'outbound', 'outcom', 'outgo', 'outlin', 'outlook', 'outofthebox', 'output', 'outreach', 'outsid', 'outsourc', 'outstand', 'over', 'overal', 'overarch', 'overcom', 'overlap', 'overse', 'oversea', 'oversight', 'own', 'owner', 'ownership', 'pace', 'pacif', 'packag', 'page', 'paid', 'pain', 'pair', 'paper', 'parallel', 'paramet', 'parcel', 'parent', 'part', 'partern', 'parti', 'particip', 'particular', 'particularli', 'partit', 'partner', 'partnercustomerfirst', 'partnerfac', 'partnerfield', 'partneroem', 'partnerscli', 'partnerscustom', 'partnership', 'partnershipretail', 'partnershipsrelev', 'partnersservic', 'passion', 'passiv', 'path', 'pattern', 'pay', 'payforperform', 'payment', 'payrol', 'pb', 'pcb', 'pdm', 'peer', 'penetr', 'peopl', 'peoplerel', 'pep', 'per', 'percept', 'perf', 'perform', 'performanceenhanc', 'performancemanag', 'period', 'peripher', 'perk', 'person', 'persona', 'personæ', 'perspect', 'persuad', 'pertain', 'pertin', 'phase', 'phenomen', 'phenomena', 'philosophi', 'phone', 'phoneemail', 'physic', 'physicalmanufactur', 'pick', 'pictur', 'piec', 'pillar', 'pilot', 'pip', 'pipelin', 'pippep', 'pitch', 'pixel', 'pixelperfect', 'place', 'placement', 'plan', 'planner', 'planningfin', 'platform', 'platformlevel', 'play', 'playbook', 'playersperson', 'pleas', 'pm', 'pmengin', 'pmm', 'poc', 'pod', 'podregion', 'poignant', 'point', 'pointofcontact', 'polic', 'polici', 'policiesexcept', 'policymak', 'policyrel', 'polish', 'polit', 'politician', 'pool', 'pop', 'popul', 'poros', 'portal', 'portfolio', 'posit', 'possess', 'possibl', 'post', 'postbeta', 'postbrief', 'postev', 'postlaunch', 'postmortem', 'postsal', 'postur', 'potenti', 'power', 'pqp', 'practic', 'practition', 'pragmat', 'pre', 'predevelop', 'predict', 'preemptiv', 'prefer', 'prelaunch', 'preliminari', 'prepar', 'prepared', 'prepay', 'prepostprocess', 'presal', 'prescript', 'presenc', 'present', 'presentimpl', 'preserv', 'presilicon', 'press', 'pressur', 'prevent', 'previou', 'price', 'primari', 'primarili', 'principl', 'print', 'priorit', 'prioriti', 'privaci', 'privat', 'prm', 'proactiv', 'problem', 'procedur', 'process', 'processestechnolog', 'processor', 'processproduct', 'procur', 'produc', 'producersproject', 'product', 'productapi', 'productengin', 'production', 'productionlevel', 'productionreadi', 'productprocess', 'productprogram', 'productsfeatur', 'productsservic', 'productssolut', 'producttechn', 'profession', 'professor', 'profici', 'profil', 'profit', 'program', 'programlogist', 'programm', 'programmat', 'progress', 'progressdependenciesrisksissu', 'progressstatu', 'project', 'projectagr', 'projectrel', 'projectsin', 'promis', 'promot', 'prompt', 'promptli', 'proof', 'proofofconcept', 'proofread', 'proofsofconcept', 'proper', 'properli', 'properti', 'propos', 'proposeddraft', 'proposit', 'proprietari', 'prospect', 'protect', 'protocol', 'prototyp', 'prototypesmodel', 'proven', 'provid', 'provis', 'pse', 'psm', 'psopartn', 'psychosoci', 'public', 'publish', 'publisherinventori', 'publishersdevelop', 'pull', 'puls', 'purchas', 'pursu', 'push', 'put', 'pwa', 'python', 'qa', 'qbr', 'qor', 'qualifi', 'qualit', 'qualiti', 'qualityspe', 'quantifi', 'quantit', 'quantiti', 'quarter', 'quarterli', 'queri', 'question', 'queue', 'quick', 'quickli', 'quo', 'quota', 'quotat', 'rais', 'ramp', 'rang', 'rapid', 'rapidli', 'rate', 'ratio', 'ration', 'rational', 'rc', 'rcscommun', 'reach', 'react', 'reactiv', 'read', 'readabl', 'readi', 'readinessskil', 'real', 'realist', 'realiti', 'realiz', 'reason', 'receiv', 'recept', 'recogn', 'recognit', 'recommend', 'reconcil', 'reconcili', 'record', 'recoveri', 'recruit', 'rectifi', 'recur', 'recurr', 'redesign', 'redirect', 'reduc', 'reduct', 'reengin', 'refer', 'referr', 'refin', 'reflect', 'refresh', 'regard', 'region', 'regionalincountri', 'regionterritori', 'regist', 'registr', 'registrationlead', 'regul', 'regular', 'regularli', 'regulationsrequir', 'regulatori', 'rehears', 'reject', 'rel', 'relat', 'relationship', 'relationshipbas', 'relationshipbuild', 'releas', 'relev', 'reliabl', 'remain', 'remedi', 'remot', 'remov', 'render', 'renew', 'rent', 'reorgan', 'repair', 'repatri', 'repeat', 'replic', 'report', 'repositori', 'repres', 'repriorit', 'reproduc', 'reput', 'request', 'requir', 'requisit', 'research', 'resel', 'reserv', 'residenti', 'residu', 'resili', 'resist', 'resolut', 'resolv', 'reson', 'resourc', 'respect', 'respond', 'respons', 'rest', 'restrict', 'restructur', 'result', 'resum', 'retail', 'retain', 'retent', 'retriev', 'return', 'reusabl', 'reveal', 'revenu', 'revers', 'review', 'revis', 'rew', 'reward', 'rfi', 'rfirfp', 'rfp', 'rfq', 'rfx', 'rhythm', 'rich', 'right', 'rigor', 'risk', 'riskbas', 'riskreturn', 'road', 'roadblock', 'roadmap', 'robot', 'robotstxt', 'robust', 'roi', 'role', 'roll', 'rollout', 'room', 'root', 'rootcaus', 'rotat', 'rout', 'router', 'routin', 'rtl', 'rtldesign', 'rule', 'run', 'russia', 'russian', 'saa', 'safe', 'safeti', 'salari', 'sale', 'salesact', 'salesensur', 'salesforc', 'salesoper', 'salespartn', 'samestoregrowth', 'sampl', 'sap', 'sarbanesoxley', 'satisfact', 'satisfi', 'save', 'scalabl', 'scale', 'scenario', 'schedul', 'schemat', 'scienc', 'scientist', 'scope', 'score', 'scorecard', 'screen', 'script', 'sculpt', 'sdm', 'sea', 'seamless', 'search', 'season', 'seat', 'sector', 'secur', 'securityrisk', 'seek', 'segment', 'select', 'selflearn', 'selfservic', 'selfsustain', 'sell', 'sellin', 'sellout', 'seminar', 'senior', 'seniorlevel', 'sens', 'sensecheck', 'sensit', 'sensor', 'sent', 'separ', 'seri', 'serv', 'server', 'servic', 'session', 'set', 'setup', 'sever', 'sfdc', 'shape', 'share', 'sheet', 'shepherd', 'shift', 'ship', 'shop', 'short', 'shot', 'show', 'showcas', 'side', 'sidebysid', 'sigma', 'signal', 'signific', 'signoff', 'signup', 'similar', 'simpl', 'simpler', 'simpli', 'simplifi', 'simul', 'simultan', 'singl', 'site', 'siteinfrastructur', 'sitelevel', 'siterel', 'situat', 'six', 'size', 'sketch', 'skill', 'sla', 'slide', 'small', 'smart', 'smarter', 'smb', 'sme', 'smooth', 'smoother', 'smoothli', 'smt', 'soc', 'social', 'softwar', 'sold', 'solid', 'solidifi', 'solut', 'solutionori', 'solutionsconfigur', 'solutionsimplement', 'solutionsin', 'solutionsori', 'solv', 'sometim', 'sophist', 'sound', 'soundingboardspar', 'sourc', 'sourcer', 'south', 'southeast', 'sox', 'space', 'spaceposit', 'spam', 'span', 'spawn', 'speak', 'speaker', 'spearhead', 'special', 'specialist', 'specif', 'specifi', 'specificationsguidelin', 'spectrum', 'speech', 'speed', 'spend', 'spirit', 'spokespeopl', 'spokesperson', 'sponsor', 'sponsorship', 'sport', 'spread', 'sprint', 'sql', 'sre', 'ssale', 'ssp', 'sstakehold', 'stabil', 'stabl', 'staf', 'staff', 'staffer', 'staffinghr', 'stage', 'stakehold', 'stand', 'standalon', 'standard', 'standardcompli', 'standardsdriven', 'start', 'startup', 'state', 'statement', 'static', 'statist', 'statu', 'statusprogress', 'statutori', 'stay', 'steer', 'stellar', 'step', 'steward', 'stewardship', 'stimul', 'stop', 'storag', 'store', 'stori', 'storyboard', 'storylin', 'storytel', 'strateg', 'strategi', 'strategicsenior', 'strategist', 'strategybas', 'strategyop', 'stream', 'streamlin', 'strength', 'strengthen', 'stress', 'strive', 'strong', 'structur', 'student', 'studi', 'studio', 'studyaudit', 'stun', 'style', 'sub', 'subject', 'subjectmatt', 'submiss', 'submit', 'subpoena', 'subprocessor', 'subregion', 'subscript', 'subsequ', 'substant', 'substanti', 'subsystem', 'subteam', 'subvert', 'success', 'suffici', 'suggest', 'suit', 'suitabl', 'summar', 'summari', 'summit', 'sunset', 'supervis', 'supplement', 'suppli', 'supplier', 'supplierrel', 'suppliersmanufactur', 'supplydemand', 'support', 'supportattend', 'supportdr', 'supportgradu', 'sure', 'surfac', 'survey', 'suspect', 'suspens', 'sustain', 'svmg', 'switch', 'synchron', 'syndic', 'synthes', 'synthesi', 'system', 'systemat', 'tabl', 'tablet', 'tact', 'tactic', 'tag', 'tailor', 'taipeibas', 'taiwan', 'take', 'takeaway', 'talent', 'talk', 'tam', 'target', 'targetsgo', 'task', 'tax', 'teach', 'team', 'teammat', 'teamsth', 'teamsupport', 'teamwid', 'teas', 'tech', 'technic', 'technicalproductproject', 'technician', 'techniqu', 'technolog', 'technologist', 'technologyrel', 'telco', 'telecom', 'telecommun', 'tell', 'temp', 'templat', 'temporari', 'tenet', 'term', 'territori', 'test', 'testabl', 'testrepair', 'text', 'textur', 'thai', 'theatric', 'theft', 'theme', 'theoret', 'theori', 'therebi', 'theyr', 'thi', 'thing', 'think', 'third', 'thirdparti', 'thorough', 'thoroughli', 'though', 'thought', 'thoughtleadership', 'threat', 'three', 'thrive', 'throughout', 'throughput', 'ticket', 'tie', 'tier', 'tight', 'tightli', 'time', 'timeli', 'timelin', 'timelinesschedul', 'timesensit', 'timezon', 'tla', 'today', 'togeth', 'toler', 'tone', 'tool', 'toolkit', 'toolsservic', 'toolstempl', 'top', 'topic', 'topicsthem', 'topmanag', 'total', 'touch', 'tough', 'toward', 'toy', 'toyota', 'track', 'traction', 'trade', 'trademark', 'tradeoff', 'tradit', 'traffic', 'train', 'trainer', 'trajectori', 'transact', 'transfer', 'transform', 'transit', 'translat', 'transpar', 'transport', 'travel', 'treasuri', 'treat', 'treatment', 'trend', 'tri', 'triag', 'trial', 'tricki', 'trip', 'troubl', 'troubleshoot', 'true', 'trust', 'trustbuild', 'trustworthi', 'tune', 'turn', 'tutori', 'tvc', 'type', 'typic', 'typographi', 'uat', 'ui', 'uiux', 'ukrain', 'uku', 'ultim', 'unanticip', 'uncov', 'underli', 'underpin', 'understand', 'understood', 'unifi', 'uniform', 'uniqu', 'unit', 'uniti', 'univers', 'unlock', 'unmet', 'unplan', 'unpreced', 'unscript', 'unstructur', 'upcom', 'updat', 'updatesresult', 'upfront', 'upgrad', 'uplevel', 'uplift', 'upload', 'upon', 'upper', 'upscal', 'upskil', 'upstream', 'uptim', 'uptod', 'urban', 'urgent', 'us', 'usa', 'usabl', 'usag', 'usagerepurchas', 'use', 'usecas', 'user', 'usercar', 'usercentr', 'userfac', 'userfocus', 'userimpact', 'userinterfac', 'usher', 'util', 'utmost', 'utterli', 'uxr', 'valid', 'valu', 'valuabl', 'valuat', 'valuead', 'valueadd', 'vandal', 'vari', 'variabl', 'varianc', 'varieti', 'variou', 'vc', 'vector', 'vehicl', 'veloc', 'velocityproduct', 'vend', 'vendor', 'verbal', 'verif', 'verifi', 'verilog', 'vers', 'versatil', 'version', 'versu', 'vertic', 'vet', 'via', 'vibrant', 'video', 'videoscom', 'vietnames', 'view', 'violat', 'vipinfluenc', 'virtual', 'visa', 'visibl', 'vision', 'visionari', 'visit', 'visual', 'visualverb', 'voic', 'volum', 'volumevalu', 'vp', 'vs', 'vui', 'vulner', 'wareh', 'warehous', 'warm', 'warn', 'warranti', 'watch', 'way', 'waze', 'weak', 'web', 'webcast', 'webpagetest', 'websit', 'websiteapp', 'week', 'weekend', 'weekli', 'weight', 'well', 'wellcoordin', 'wellfocus', 'wellground', 'wellinform', 'wellthought', 'wherev', 'whilst', 'white', 'whiteboard', 'whitepap', 'whole', 'wide', 'wider', 'will', 'willing', 'win', 'window', 'windowsbas', 'wirefram', 'wireless', 'within', 'without', 'work', 'workaround', 'worker', 'workflow', 'workflowsprocess', 'workforc', 'workload', 'workplac', 'workshop', 'workstream', 'world', 'worldclass', 'worldlead', 'worldwid', 'worthi', 'would', 'wrapup', 'write', 'writedevelop', 'writer', 'written', 'xfunction', 'yard', 'yearend', 'yearli', 'yield', 'youtub', 'yt4p', 'zone', 'などの', 'を設計', 'オープンソース', 'グラフィック', 'グラフィックのフレームワークを作成する', 'コミュニティと連携する', 'ソフトウェアの低レベル開発に取り組む', 'デバイス', 'ドライバに加え', 'パートナー様', 'ベースシステム用の動画', 'メディア', '動画', '新しいマルチプラットフォームの基準や', '社内外のソフトウェア担当チームやハードウェア担当チーム', '開発し']\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf_vect.fit_transform(data2['Responsibilities'])\n",
    "print(X_tfidf.shape)\n",
    "print(type(X_tfidf))\n",
    "print(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, I create a dataframe with the values of the TF-IDF matrix that will be needed for creating my training and test dataset. The feature names of the matrix are asigned as columns to the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code will import the utility method train_test_split of sklearn.model_selection that will help me divide my dataset in training and testing part. Also precision_recall_fscore_support is imported in order to get the performance scores of my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_df, data2['My_category'], test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My problem is a multiclass classification and I will use a RandomForestClassifier \n",
    "#### The RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=None, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cross validation will help detect if there is a variance problem with the accuracy. The variance problem refers to the case in which the accuracy obtained on one test set is very different to the accuracy obtained on another test set using the same algorithm. This behavior is a problem as it brings uncertainty into the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_5_accuracies = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5, scoring='accuracy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74752475, 0.76      , 0.8071066 , 0.72959184, 0.73056995])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_5_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fold_5_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean and std of accuracy variance = 0.7549586272762483,0.028419608079388124\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean and std of accuracy variance = \"+str(fold_5_accuracies.mean())+\",\"+str(fold_5_accuracies.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The standard deviation of accuracy is pretty small and this means that the model has a small variance. This means also that the prediction I will obtain on the test set will not be by chance but it will be varying only a little bit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model and find out most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.011155791916469149, 'intern'),\n",
       " (0.010685602712047293, 'partner'),\n",
       " (0.009454629774904175, 'respons'),\n",
       " (0.009187888516980732, 'opportun'),\n",
       " (0.008916603822282078, 'market'),\n",
       " (0.008814977471989107, 'technic'),\n",
       " (0.008345585296317229, 'design'),\n",
       " (0.008130482958069306, 'develop'),\n",
       " (0.008124453740652264, 'account'),\n",
       " (0.007781174826090262, 'busi'),\n",
       " (0.0076280843350508476, 'financi'),\n",
       " (0.007622032244858429, 'give'),\n",
       " (0.007134267731876356, 'custom'),\n",
       " (0.007125677748699116, 'strateg'),\n",
       " (0.007064061515315377, 'googl'),\n",
       " (0.007003574708178064, 'product'),\n",
       " (0.006808162703409063, 'user'),\n",
       " (0.00618588045333642, 'work'),\n",
       " (0.00603626047407941, 'candid'),\n",
       " (0.005974680631710202, 'relationship')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse = True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions using measures like precision, recall, fscore \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <img src=\"files/Precisionrecall.svg.png\" alt=\"PrecisionRecall\" height=\"250\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code will display the measures of precision, recall and fscore for every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales & Account Management ===> Precision :0.838 / Recall : 0.939 / Fscore : 0.886 \n",
      "\n",
      "Marketing & Communications ===> Precision :0.81 / Recall : 0.944 / Fscore : 0.872 \n",
      "\n",
      "Finance ===> Precision :0.882 / Recall : 0.789 / Fscore : 0.833 \n",
      "\n",
      "Technical Solutions ===> Precision :0.905 / Recall : 0.905 / Fscore : 0.905 \n",
      "\n",
      "Business Strategy ===> Precision :0.889 / Recall : 0.762 / Fscore : 0.821 \n",
      "\n",
      "People Operations ===> Precision :0.952 / Recall : 0.909 / Fscore : 0.93 \n",
      "\n",
      "User Experience & Design ===> Precision :1.0 / Recall : 0.706 / Fscore : 0.828 \n",
      "\n",
      "Program Management ===> Precision :1.0 / Recall : 0.727 / Fscore : 0.842 \n",
      "\n",
      "Partnerships ===> Precision :0.8 / Recall : 0.667 / Fscore : 0.727 \n",
      "\n",
      "Product & Customer Support ===> Precision :1.0 / Recall : 0.3 / Fscore : 0.462 \n",
      "\n",
      "Other ===> Precision :0.741 / Recall : 0.956 / Fscore : 0.835 \n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    print() \n",
    "    print(str(categories[i])+\" ===> \"+'Precision :{} / Recall : {} / Fscore : {} '.format(round(precision[i],3), \n",
    "                                                                                          round(recall[i],3), \n",
    "                                                                                          round(fscore[i],3),\n",
    "                                                                                          \n",
    "          )\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = round((y_pred == y_test).sum() / len(y_pred) , 3)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The measure that I'm trying to improve is F-score. I find this measure is important as it expresses precision and recall in a synthetic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est:10 / Depth: 10 / Pre :0.6842105263157895 / Rec : 0.7878787878787878 / Fscore : 0.732394366197183 / Acc.: 0.619 \n",
      "Est:10 / Depth: 30 / Pre :0.7837837837837838 / Rec : 0.8787878787878788 / Fscore : 0.8285714285714285 / Acc.: 0.785 \n",
      "Est:10 / Depth: 50 / Pre :0.7435897435897436 / Rec : 0.8787878787878788 / Fscore : 0.8055555555555556 / Acc.: 0.781 \n",
      "Est:10 / Depth: 150 / Pre :0.7435897435897436 / Rec : 0.8787878787878788 / Fscore : 0.8055555555555556 / Acc.: 0.773 \n",
      "Est:10 / Depth: None / Pre :0.7894736842105263 / Rec : 0.9090909090909091 / Fscore : 0.8450704225352113 / Acc.: 0.785 \n",
      "---------------------------------------------------\n",
      "Est:30 / Depth: 10 / Pre :0.6511627906976745 / Rec : 0.8484848484848485 / Fscore : 0.736842105263158 / Acc.: 0.664 \n",
      "Est:30 / Depth: 30 / Pre :0.8157894736842105 / Rec : 0.9393939393939394 / Fscore : 0.8732394366197183 / Acc.: 0.81 \n",
      "Est:30 / Depth: 50 / Pre :0.7837837837837838 / Rec : 0.8787878787878788 / Fscore : 0.8285714285714285 / Acc.: 0.822 \n",
      "Est:30 / Depth: 150 / Pre :0.775 / Rec : 0.9393939393939394 / Fscore : 0.8493150684931509 / Acc.: 0.822 \n",
      "Est:30 / Depth: None / Pre :0.8108108108108109 / Rec : 0.9090909090909091 / Fscore : 0.8571428571428571 / Acc.: 0.806 \n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est:50 / Depth: 10 / Pre :0.7567567567567568 / Rec : 0.8484848484848485 / Fscore : 0.8000000000000002 / Acc.: 0.688 \n",
      "Est:50 / Depth: 30 / Pre :0.8333333333333334 / Rec : 0.9090909090909091 / Fscore : 0.8695652173913043 / Acc.: 0.814 \n",
      "Est:50 / Depth: 50 / Pre :0.7692307692307693 / Rec : 0.9090909090909091 / Fscore : 0.8333333333333333 / Acc.: 0.826 \n",
      "Est:50 / Depth: 150 / Pre :0.8823529411764706 / Rec : 0.9090909090909091 / Fscore : 0.8955223880597014 / Acc.: 0.826 \n",
      "Est:50 / Depth: None / Pre :0.8157894736842105 / Rec : 0.9393939393939394 / Fscore : 0.8732394366197183 / Acc.: 0.834 \n",
      "---------------------------------------------------\n",
      "Est:100 / Depth: 10 / Pre :0.7631578947368421 / Rec : 0.8787878787878788 / Fscore : 0.8169014084507042 / Acc.: 0.7 \n",
      "Est:100 / Depth: 30 / Pre :0.9117647058823529 / Rec : 0.9393939393939394 / Fscore : 0.9253731343283583 / Acc.: 0.826 \n",
      "Est:100 / Depth: 50 / Pre :0.7837837837837838 / Rec : 0.8787878787878788 / Fscore : 0.8285714285714285 / Acc.: 0.822 \n",
      "Est:100 / Depth: 150 / Pre :0.8285714285714286 / Rec : 0.8787878787878788 / Fscore : 0.8529411764705883 / Acc.: 0.814 \n",
      "Est:100 / Depth: None / Pre :0.8157894736842105 / Rec : 0.9393939393939394 / Fscore : 0.8732394366197183 / Acc.: 0.822 \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators = n_est, max_depth = depth, n_jobs = -1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision,recall,fscore,support = score(y_test, y_pred)\n",
    "    print('Est:{} / Depth: {} / Pre :{} / Rec : {} / Fscore : {} / Acc.: {} '.format(\n",
    "          n_est, depth, precision[0], recall[0], fscore[0], round((y_pred == y_test).sum() / len(y_pred) , 3)\n",
    "          )) \n",
    "    \n",
    "for n_est in [10,30, 50, 100]:\n",
    "    for depth in [10,30, 50, 150, None]:\n",
    "        train_RF(n_est, depth)\n",
    "    print('---------------------------------------------------')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best parameters for my model through grid searching and tuning can be a long process . There is a better way to do the things and this is  by using GridSearchCV class of sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_par = {  \n",
    "    'n_estimators': [50, 100, 300, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_sr = GridSearchCV(estimator=rf_model,  \n",
    "                     param_grid=grid_par,\n",
    "                     scoring='f1_weighted',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [50, 100, 300, 500], 'criterion': ['gini', 'entropy'], 'bootstrap': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sr.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'criterion': 'gini', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "best_param= gd_sr.best_params_  \n",
    "print(best_param)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  5,  3,  6,  5,  3,  8,  8,  9, 10,  3,  0,  3,  1,  8,\n",
       "        0,  0, 10,  9,  3,  1,  4, 10, 10, 10,  2,  3,  1,  1,  4,  0,  1,\n",
       "       10,  1,  2,  1, 10,  0,  0, 10,  3,  5,  0,  2,  1, 10, 10,  6, 10,\n",
       "        0,  1,  3,  4,  1,  2, 10,  6,  6,  1, 10,  0,  5,  4,  2,  0,  8,\n",
       "       10,  3,  4,  5, 10, 10,  2,  4,  2,  1, 10,  8,  7,  0,  6,  3,  3,\n",
       "        3, 10,  5,  1,  2,  5,  0,  1,  3, 10,  6,  0, 10,  1,  0,  4,  0,\n",
       "        1,  3,  5,  5,  3,  9,  6,  7, 10,  0, 10, 10,  1,  5,  4,  0,  1,\n",
       "        4, 10,  1,  1,  6,  5,  1,  7, 10,  8,  2,  8,  6, 10,  1, 10, 10,\n",
       "        0,  5, 10,  0,  3, 10, 10,  2,  7, 10,  1,  3, 10,  1,  2, 10, 10,\n",
       "        0,  8,  1,  0,  1,  3,  1,  1,  4,  7, 10,  4,  4,  0, 10,  0,  3,\n",
       "        7,  0, 10,  5,  7, 10,  5,  0,  0,  2,  0,  1,  4,  1,  5,  3,  4,\n",
       "        1, 10, 10,  0,  1,  0,  2,  1,  4,  4, 10, 10,  1, 10,  4,  2,  3,\n",
       "        5,  2,  0, 10,  8,  2,  5,  0, 10,  1,  8, 10, 10, 10, 10,  1,  6,\n",
       "        2, 10,  5,  0, 10,  4, 10, 10,  0,  0,  1,  1,  1,  4,  1,  5, 10,\n",
       "        6,  0,  5,  2,  0, 10,  5, 10,  6])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that I have the best hyperparameter I can check and see if the predictions improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bis = gd_sr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_test, y_pred_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales & Account Management b ===> Precision :0.789 / Recall : 0.909 / Fscore : 0.845 \n",
      "\n",
      "Marketing & Communications b ===> Precision :0.854 / Recall : 0.972 / Fscore : 0.909 \n",
      "\n",
      "Finance b ===> Precision :0.778 / Recall : 0.737 / Fscore : 0.757 \n",
      "\n",
      "Technical Solutions b ===> Precision :0.905 / Recall : 0.905 / Fscore : 0.905 \n",
      "\n",
      "Business Strategy b ===> Precision :0.842 / Recall : 0.762 / Fscore : 0.8 \n",
      "\n",
      "People Operations b ===> Precision :0.952 / Recall : 0.909 / Fscore : 0.93 \n",
      "\n",
      "User Experience & Design b ===> Precision :1.0 / Recall : 0.706 / Fscore : 0.828 \n",
      "\n",
      "Program Management b ===> Precision :1.0 / Recall : 0.636 / Fscore : 0.778 \n",
      "\n",
      "Partnerships b ===> Precision :0.8 / Recall : 0.667 / Fscore : 0.727 \n",
      "\n",
      "Product & Customer Support b ===> Precision :1.0 / Recall : 0.3 / Fscore : 0.462 \n",
      "\n",
      "Other b ===> Precision :0.754 / Recall : 0.956 / Fscore : 0.843 \n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    print()\n",
    "    print(str(categories[i])+\" b ===> \"+'Precision :{} / Recall : {} / Fscore : {} '.format(\n",
    "           round(precision[i],3), round(recall[i],3), round(fscore[i],3)\n",
    "          )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to check if the measure Fscore improves by using GridSearchCV I confront the Fscore value that I get with GridSearchCV with the values I obtained previously "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous results : screenshot \n",
    "<img src=\"files/previousresults.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results from confrontation between the two versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using GridSearchCV the Fscore measure improves in 8 out of 11 categories. For the remaining 3 categories the Fscore worsens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
