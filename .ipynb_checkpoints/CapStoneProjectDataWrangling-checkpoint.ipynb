{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the analysis is a decription of dataset following the EDA (Explorative Data Analysis) showing\n",
    "# the columns or variables, the dimension of the dataset in rows and columns, the first five rows,\n",
    "# I have also checked if there are null values which\n",
    "# decided to delete. The reason why I decided to delete them is the impossibility to subsitute them with\n",
    "# the \"mean\" giben the fact that I'm analyzing text data.\n",
    "\n",
    "# here import the necessary libraries \n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas.compat import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data file\n",
    "with open('job_skills.csv',  'r') as myfile:\n",
    "  jobs = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to dataframe. I will convert it to dataframe as it is the most useful format for doing EDA (Exploratory data analysis)\n",
    "data_jobs = pd.read_csv(StringIO(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - the columns of the dataset \n",
      "Index(['Company', 'Title', 'Category', 'Location', 'Responsibilities',\n",
      "       'Minimum Qualifications', 'Preferred Qualifications'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Step 1 - the columns of the dataset ')\n",
    "print(data_jobs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - the shape of the data in rows and columns\n",
      "shape of the datasets =  (1250, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Step 2 - the shape of the data in rows and columns')\n",
    "print('shape of the datasets = ',data_jobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 - the first five rows of the data\n",
      "  Company                                              Title  \\\n",
      "0  Google                       Google Cloud Program Manager   \n",
      "1  Google  Supplier Development Engineer (SDE), Cable/Con...   \n",
      "2  Google  Data Analyst, Product and Tools Operations, Go...   \n",
      "3  Google            Developer Advocate, Partner Engineering   \n",
      "4  Google     Program Manager, Audio Visual (AV) Deployments   \n",
      "\n",
      "                       Category                          Location  \\\n",
      "0            Program Management                         Singapore   \n",
      "1  Manufacturing & Supply Chain                   Shanghai, China   \n",
      "2           Technical Solutions       New York, NY, United States   \n",
      "3           Developer Relations  Mountain View, CA, United States   \n",
      "4            Program Management      Sunnyvale, CA, United States   \n",
      "\n",
      "                                    Responsibilities  \\\n",
      "0  Shape, shepherd, ship, and show technical prog...   \n",
      "1  Drive cross-functional activities in the suppl...   \n",
      "2  Collect and analyze data to draw insight and i...   \n",
      "3  Work one-on-one with the top Android, iOS, and...   \n",
      "4  Plan requirements with internal customers.\\nPr...   \n",
      "\n",
      "                              Minimum Qualifications  \\\n",
      "0  BA/BS degree or equivalent practical experienc...   \n",
      "1  BS degree in an Engineering discipline or equi...   \n",
      "2  Bachelorâ€™s degree in Business, Economics, Stat...   \n",
      "3  BA/BS degree in Computer Science or equivalent...   \n",
      "4  BA/BS degree or equivalent practical experienc...   \n",
      "\n",
      "                            Preferred Qualifications  \n",
      "0  Experience in the business technology market a...  \n",
      "1  BSEE, BSME or BSIE degree.\\nExperience of usin...  \n",
      "2  Experience partnering or consulting cross-func...  \n",
      "3  Experience as a software developer, architect,...  \n",
      "4  CTS Certification.\\nExperience in the construc...  \n"
     ]
    }
   ],
   "source": [
    "print('Step 3 - the first five rows of the data')\n",
    "print(data_jobs.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 - Check if there are null values\n",
      "There are null variables =  Company                      0\n",
      "Title                        0\n",
      "Category                     0\n",
      "Location                     0\n",
      "Responsibilities            15\n",
      "Minimum Qualifications      14\n",
      "Preferred Qualifications    14\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Step 4 - Check if there are null values')\n",
    "print('There are null variables = ', data_jobs.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 - Delete eventual null values\n",
      "Final shape of the dataset after deleting rows with null values \n",
      "After deleting 15 rows containing null values the new dimension of the dataset is as follows :  (1235, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Step 5 - Delete eventual null values')\n",
    "data_jobs = data_jobs.dropna()\n",
    "print('Final shape of the dataset after deleting rows with null values ')\n",
    "print('After deleting 15 rows containing null values the new dimension of the dataset is as follows : ',data_jobs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the process of text analysis goes through the followng steps :\n",
    "# 1-tokenize the text in single words\n",
    "# 2-put all the words in lower case\n",
    "# 3- remove stop words such as 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'you', \"you're\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a tokenizer with RegexpTokenizer and use it to create tokens (words) from the text\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "tokens = tokenizer.tokenize(jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list where to put the lower case tokens\n",
    "words = []\n",
    "\n",
    "# Loop through list tokens and make lower case\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords such as 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"\n",
    "# and print them\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "print(sw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list that will contain text without stop words\n",
    "words_ns = []\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'title', 'category', 'location', 'responsibilities', 'minimum', 'qualifications', 'preferred', 'qualifications', 'google', 'google', 'cloud', 'program', 'manager', 'program', 'management', 'singapore', 'shape', 'shepherd', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Print several list items as sanity check\n",
    "print(words_ns[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frequency of the words \n",
    "# adjust the plot margins.\n",
    "plt.subplots_adjust(left=0.15, bottom=0.25, right=0.9, top=0.8)\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "# matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Create freq dist and plot\n",
    "freqdist1 = nltk.FreqDist(words_ns)\n",
    "freqdist1.plot(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
